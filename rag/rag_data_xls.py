import pandas as pd
import os
import glob
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from .llm import VECTOR_STORE, EMBEDDINGS

from typing import List, Dict, Any, Optional
import hashlib

import os
from dotenv import find_dotenv, load_dotenv
import getpass


class XLSXLoader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ XLSX —Ñ–∞–π–ª–æ–≤ –≤ ChromaDB"""
    
    def __init__(self, vector_store: Optional[Chroma] = None, 
                 chunk_size: int = 1000, 
                 chunk_overlap: int = 200):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
        
        Args:
            vector_store: ChromaDB –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        """
        self.vector_store = vector_store or VECTOR_STORE
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # –°–æ–∑–¥–∞–µ–º text splitter —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def load_xlsx_to_documents(self, file_path: str, processing_mode: str = "rows") -> List[Document]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç XLSX —Ñ–∞–π–ª –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ Document –æ–±—ä–µ–∫—Ç—ã
        
        Args:
            file_path: –ü—É—Ç—å –∫ XLSX —Ñ–∞–π–ª—É
            processing_mode: –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ ('rows', 'columns', 'cells', 'sheets')
                - 'rows': –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ = –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                - 'columns': –∫–∞–∂–¥–∞—è –∫–æ–ª–æ–Ω–∫–∞ = –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                - 'cells': –∫–∞–∂–¥–∞—è —è—á–µ–π–∫–∞ = –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                - 'sheets': –∫–∞–∂–¥—ã–π –ª–∏—Å—Ç = –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        
        Returns:
            –°–ø–∏—Å–æ–∫ Document –æ–±—ä–µ–∫—Ç–æ–≤
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã Excel —Ñ–∞–π–ª–∞
        excel_data = pd.read_excel(file_path, sheet_name=None)
        documents = []
        
        for sheet_name, df in excel_data.items():
            print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ª–∏—Å—Ç: {sheet_name}")
            
            if processing_mode == "sheets":
                documents.extend(self._process_sheet_mode(df, sheet_name, file_path))
            elif processing_mode == "rows":
                documents.extend(self._process_rows_mode(df, sheet_name, file_path))
            elif processing_mode == "columns":
                documents.extend(self._process_columns_mode(df, sheet_name, file_path))
            elif processing_mode == "cells":
                documents.extend(self._process_cells_mode(df, sheet_name, file_path))
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_mode}")
        
        return documents
    
    def _process_sheet_mode(self, df: pd.DataFrame, sheet_name: str, file_path: str) -> List[Document]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–µ—Å—å –ª–∏—Å—Ç –∫–∞–∫ –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç"""
        content = f"–õ–∏—Å—Ç: {sheet_name}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        content += "–ó–∞–≥–æ–ª–æ–≤–∫–∏: " + ", ".join(df.columns.astype(str)) + "\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
        for index, row in df.iterrows():
            row_data = []
            for col_name, value in row.items():
                if pd.notna(value):
                    row_data.append(f"{col_name}: {value}")
            
            if row_data:
                content += f"–°—Ç—Ä–æ–∫–∞ {index + 1}: " + "; ".join(row_data) + "\n"
        
        return [Document(
            page_content=content,
            metadata={
                "source": file_path,
                "sheet_name": sheet_name,
                "type": "excel_sheet",
                "rows_count": len(df),
                "columns_count": len(df.columns)
            }
        )]
    
    def _process_rows_mode(self, df: pd.DataFrame, sheet_name: str, file_path: str) -> List[Document]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"""
        documents = []
        
        for index, row in df.iterrows():
            row_data = []
            for col_name, value in row.items():
                if pd.notna(value):
                    row_data.append(f"{col_name}: {value}")
            
            if row_data:
                content = f"–°—Ç—Ä–æ–∫–∞ {index + 1} –∏–∑ –ª–∏—Å—Ç–∞ '{sheet_name}':\n" + "\n".join(row_data)
                
                documents.append(Document(
                    page_content=content,
                    metadata={
                        "source": file_path,
                        "sheet_name": sheet_name,
                        "row_index": index + 1,
                        "type": "excel_row"
                    }
                ))
        
        return documents
    
    def _process_columns_mode(self, df: pd.DataFrame, sheet_name: str, file_path: str) -> List[Document]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥—É—é –∫–æ–ª–æ–Ω–∫—É –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"""
        documents = []
        
        for col_name in df.columns:
            values = df[col_name].dropna().astype(str).tolist()
            
            if values:
                content = f"–ö–æ–ª–æ–Ω–∫–∞ '{col_name}' –∏–∑ –ª–∏—Å—Ç–∞ '{sheet_name}':\n"
                content += "\n".join([f"- {value}" for value in values])
                
                documents.append(Document(
                    page_content=content,
                    metadata={
                        "source": file_path,
                        "sheet_name": sheet_name,
                        "column_name": col_name,
                        "values_count": len(values),
                        "type": "excel_column"
                    }
                ))
        
        return documents

    def _process_cells_mode(self, df: pd.DataFrame, sheet_name: str, file_path: str) -> List[Document]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥—É—é —è—á–µ–π–∫—É –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"""
        documents = []
        
        for row_idx, row in df.iterrows():
            for col_name, value in row.items():
                if pd.notna(value) and str(value).strip():
                    content = f"–Ø—á–µ–π–∫–∞ {col_name}:{row_idx + 1} –∏–∑ –ª–∏—Å—Ç–∞ '{sheet_name}': {value}"
                    
                    documents.append(Document(
                        page_content=content,
                        metadata={
                            "source": file_path,
                            "sheet_name": sheet_name,
                            "row_index": row_idx + 1,
                            "column_name": col_name,
                            "type": "excel_cell"
                        }
                    ))
        
        return documents
    
    def split_documents_into_chunks(self, documents: List[Document], 
                                   min_chunk_size: int = 50) -> List[Document]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—è text_splitter
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            min_chunk_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ –º–µ–Ω—å—à–µ - –Ω–µ —á–∞–Ω–∫—É–µ–º)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–∞–∑–±–∏—Ç—ã—Ö –Ω–∞ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        print(f"üî™ –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ (—Ä–∞–∑–º–µ—Ä: {self.chunk_size}, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {self.chunk_overlap})")
        
        chunked_documents = []
        original_count = len(documents)
        
        for doc in documents:
            # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –º–∞–ª–µ–Ω—å–∫–∏–π, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if len(doc.page_content) <= min_chunk_size:
                chunked_documents.append(doc)
                continue
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
            if len(doc.page_content) > self.chunk_size:
                chunks = self.text_splitter.split_documents([doc])
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞–Ω–∫–∞—Ö –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                for i, chunk in enumerate(chunks):
                    chunk.metadata.update({
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "original_doc_length": len(doc.page_content),
                        "is_chunked": True
                    })
                
                chunked_documents.extend(chunks)
            else:
                # –î–æ–∫—É–º–µ–Ω—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ - –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –Ω–µ –±—ã–ª —Ä–∞–∑–±–∏—Ç
                doc.metadata.update({
                    "chunk_index": 0,
                    "total_chunks": 1,
                    "original_doc_length": len(doc.page_content),
                    "is_chunked": False
                })
                chunked_documents.append(doc)
        
        print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {original_count}")
        print(f"üìä –ü–æ—Å–ª–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è: {len(chunked_documents)}")
        
        return chunked_documents
    
    def add_to_vector_store(self, file_path: str, processing_mode: str = "rows", 
                           enable_chunking: bool = True) -> Dict[str, Any]:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç XLSX —Ñ–∞–π–ª –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ–º
        
        Args:
            file_path: –ü—É—Ç—å –∫ XLSX —Ñ–∞–π–ª—É
            processing_mode: –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            enable_chunking: –í–∫–ª—é—á–∏—Ç—å —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏
        """
        try:
            print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª: {file_path}")
            print(f"–†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_mode}")
            print(f"–ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ: {'–≤–∫–ª—é—á–µ–Ω–æ' if enable_chunking else '–æ—Ç–∫–ª—é—á–µ–Ω–æ'}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            documents = self.load_xlsx_to_documents(file_path, processing_mode)
            
            if not documents:
                return {
                    "success": False,
                    "error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞",
                    "documents_count": 0,
                    "chunks_count": 0
                }
            
            print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
            
            # –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
            if enable_chunking:
                documents = self.split_documents_into_chunks(documents)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ —Ñ–∞–π–ª–µ
            file_hash = self._get_file_hash(file_path)
            for doc in documents:
                doc.metadata.update({
                    "file_hash": file_hash,
                    "processing_mode": processing_mode,
                    "chunking_enabled": enable_chunking,
                    "chunk_size": self.chunk_size if enable_chunking else None,
                    "chunk_overlap": self.chunk_overlap if enable_chunking else None,
                    "added_at": pd.Timestamp.now().isoformat()
                })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            print("üìù –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ...")
            
            self.vector_store.add_documents(documents)
            print("‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –æ—Å–Ω–æ–≤–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é")
            
            return {
                "success": True,
                "documents_count": len(documents),
                "original_docs_count": len(self.load_xlsx_to_documents(file_path, processing_mode)) if enable_chunking else len(documents),
                "chunks_count": len(documents) if enable_chunking else 0,
                "file_path": file_path,
                "processing_mode": processing_mode,
                "chunking_enabled": enable_chunking,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "file_hash": file_hash,
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "documents_count": 0,
                "chunks_count": 0
            }
    
    def _get_file_hash(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def preview_xlsx_structure(self, file_path: str) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã XLSX —Ñ–∞–π–ª–∞
        
        Args:
            file_path: –ü—É—Ç—å –∫ XLSX —Ñ–∞–π–ª—É
        
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ñ–∞–π–ª–∞
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        excel_data = pd.read_excel(file_path, sheet_name=None)
        structure = {
            "file_path": file_path,
            "sheets_count": len(excel_data),
            "sheets": {}
        }
        
        for sheet_name, df in excel_data.items():
            structure["sheets"][sheet_name] = {
                "rows_count": len(df),
                "columns_count": len(df.columns),
                "columns": list(df.columns),
                "sample_data": df.head(3).to_dict('records') if len(df) > 0 else []
            }
        
        return structure


def manual_main():
    load_dotenv(find_dotenv())

    if "GIGACHAT_CREDENTIALS" not in os.environ:
        os.environ["GIGACHAT_CREDENTIALS"] = getpass.getpass("–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ GigaChat API: ")
    
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ XLSX —Ñ–∞–π–ª–æ–≤"""
    print("üîÑ –ó–∞–≥—Ä—É–∑—á–∏–∫ XLSX —Ñ–∞–π–ª–æ–≤ –≤ ChromaDB —Å —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ–º")
    print("=" * 60)
    
    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
    file_path = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ XLSX —Ñ–∞–π–ª—É: ").strip()
    
    if not file_path:
        print("‚ùå –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –Ω–µ —É–∫–∞–∑–∞–Ω")
        return
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
    print("\n‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è:")
    chunk_size = input("–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000): ").strip()
    chunk_size = int(chunk_size) if chunk_size.isdigit() else 1000
    
    chunk_overlap = input("–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 200): ").strip()
    chunk_overlap = int(chunk_overlap) if chunk_overlap.isdigit() else 200
    
    enable_chunking = input("–í–∫–ª—é—á–∏—Ç—å —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ? (y/n, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é y): ").strip().lower()
    enable_chunking = enable_chunking != 'n'
    
    loader = XLSXLoader(
        vector_store=VECTOR_STORE,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    try:
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä
        print("\nüìã –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∞–π–ª–∞...")
        structure = loader.preview_xlsx_structure(file_path)
        
        print(f"\n–§–∞–π–ª: {structure['file_path']}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤: {structure['sheets_count']}")
        
        for sheet_name, info in structure['sheets'].items():
            print(f"\n–õ–∏—Å—Ç '{sheet_name}':")
            print(f"  - –°—Ç—Ä–æ–∫: {info['rows_count']}")
            print(f"  - –ö–æ–ª–æ–Ω–æ–∫: {info['columns_count']}")
            print(f"  - –ö–æ–ª–æ–Ω–∫–∏: {', '.join(info['columns'][:5])}{'...' if len(info['columns']) > 5 else ''}")
        
        # –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        print("\nüîß –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        print("1. rows - –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
        print("2. sheets - –∫–∞–∂–¥—ã–π –ª–∏—Å—Ç –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
        print("3. columns - –∫–∞–∂–¥–∞—è –∫–æ–ª–æ–Ω–∫–∞ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
        print("4. cells - –∫–∞–∂–¥–∞—è —è—á–µ–π–∫–∞ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
        
        mode_choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-4) Default: 1: ").strip()
        mode_map = {"1": "rows", "2": "sheets", "3": "columns", "4": "cells"}

        if mode_choice not in mode_map:
            mode_choice = "1"
        processing_mode = mode_map[mode_choice]
        
        # –ó–∞–≥—Ä—É–∑–∫–∞
        print(f"\nüöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª –≤ —Ä–µ–∂–∏–º–µ '{processing_mode}'...")
        print(f"üî™ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è: —Ä–∞–∑–º–µ—Ä={chunk_size}, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ={chunk_overlap}")
        
        result = loader.add_to_vector_store(file_path, processing_mode, enable_chunking)
        
        if result["success"]:
            print("\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ!")
            print(f"üìä –ò—Ç–æ–≥–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {result['documents_count']}")
            
            if result.get('chunking_enabled'):
                print(f"üìÑ –ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {result.get('original_docs_count', 'N/A')}")
                print(f"üî™ –ß–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {result.get('chunks_count', 'N/A')}")
                print(f"‚öôÔ∏è –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {result.get('chunk_size')}")
                print(f"üîó –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {result.get('chunk_overlap')}")
            
            print(f"üîó –§–∞–π–ª: {result['file_path']}")
            print(f"üéØ –†–µ–∂–∏–º: {result['processing_mode']}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

def main():
    load_dotenv(find_dotenv())

    if "GIGACHAT_CREDENTIALS" not in os.environ:
        os.environ["GIGACHAT_CREDENTIALS"] = getpass.getpass("–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ GigaChat API: ")

    # Get all files from doc_files/xls/ directory
    files = glob.glob(os.path.join(os.path.dirname(os.path.dirname(__file__)), "doc_files", "xls", "*"))

    loader = XLSXLoader(
        vector_store=VECTOR_STORE,
        chunk_size=1000,
        chunk_overlap=200
    )

    for file_path in files:
        print("\nüìã –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∞–π–ª–∞...")
        structure = loader.preview_xlsx_structure(file_path)

        for sheet_name, info in structure['sheets'].items():
            print(f"\n–õ–∏—Å—Ç '{sheet_name}':")
            print(f"  - –°—Ç—Ä–æ–∫: {info['rows_count']}")
            print(f"  - –ö–æ–ª–æ–Ω–æ–∫: {info['columns_count']}")
            print(f"  - –ö–æ–ª–æ–Ω–∫–∏: {', '.join(info['columns'][:5])}{'...' if len(info['columns']) > 5 else ''}")

        processing_mode = "rows"
        result = loader.add_to_vector_store(file_path, processing_mode)

        if result["success"]:
            print("\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ!")
            print(f"üìä –ò—Ç–æ–≥–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {result['documents_count']}")
            
            if result.get('chunking_enabled'):
                print(f"üìÑ –ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {result.get('original_docs_count', 'N/A')}")
                print(f"üî™ –ß–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {result.get('chunks_count', 'N/A')}")
                print(f"‚öôÔ∏è –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {result.get('chunk_size')}")
                print(f"üîó –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {result.get('chunk_overlap')}")
            
            print(f"üîó –§–∞–π–ª: {result['file_path']}")
            print(f"üéØ –†–µ–∂–∏–º: {result['processing_mode']}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")


if __name__ == "__main__":
    main()